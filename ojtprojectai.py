# -*- coding: utf-8 -*-
"""ojtprojectAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fEsSiXwdtaC_qasV-n7wWhRdN5lE0KDt
"""

from google.colab import files
files.upload()

!sudo apt-get update && sudo apt-get install -y zstd

!pip install pandas openpyxl

!curl -fsSL https://ollama.com/install.sh | sh
import subprocess
import time
subprocess.Popen(["nohup", "ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
print("Waking up Ollama...")
time.sleep(10)
!ollama pull llama3
!ollama pull nomic-embed-text
print("Ollama is ready!")

import pandas as pd
import sqlite3

excel_file = "dataset4_hyperlinks.xlsx"

# Read Excel file
df = pd.read_excel(excel_file)

# Create SQLite database
db_name = "dataset.db"
conn = sqlite3.connect(db_name)

# Convert DataFrame to SQLite table
table_name = "dataset"
df.to_sql(table_name, conn, if_exists="replace", index=False)

conn.close()

print("Excel file successfully converted to SQLite database!")
print(f" Database name: {db_name}")
print(f" Table name: {table_name}")

conn = sqlite3.connect("dataset.db")
cursor = conn.cursor()

cursor.execute("SELECT * FROM dataset LIMIT 5")
rows = cursor.fetchall()

for row in rows:
    print(row)

conn.close()

#from google.colab import files
#files.download("dataset.db")

!pip install -U langgraph langchain_community langchain_ollama

!apt-get install -y graphviz libgraphviz-dev
!pip install pygraphviz

from typing import TypedDict, List
from langchain_core.messages import BaseMessage

# Define the state for the LangGraph
class GraphState(TypedDict):
    conversation_history: List[BaseMessage]
    application_name: str
    department: str
    # Add more slots here as needed

print(" LangGraph states defined.")

!pip install -U langgraph langchain langchain_community langchain_openai

from typing import TypedDict, Optional
from langgraph.graph import StateGraph, END
from langchain_ollama import ChatOllama
import sqlite3

from typing import TypedDict, Optional

class AgentState(TypedDict):
    user_query: str
    application_name: Optional[str]
    db_result: Optional[str]
    final_answer: Optional[str]
    search_distance: Optional[float]
    next_step: Optional[str]

llm = ChatOllama(
    model="llama3",
    temperature=0
)

import numpy as np

STRONG_THRESHOLD = 0.4
WEAK_THRESHOLD = 0.9

def search_database(state: AgentState):
    # print(" FAISS VECTOR SEARCH CALLED") # Hide this print statement

    query = state.get("application_name") or state["user_query"]

    query_embedding = embedding_model.embed_query(query)
    query_vector = np.array([query_embedding], dtype="float32")
    faiss.normalize_L2(query_vector)

    D, I = index.search(query_vector, k=3)

    best_score = D[0][0]
    indices = I[0]

    # print(f" FAISS distance score: {best_score}") # Hide this print statement

    # ---------------- STRONG MATCH ----------------
    if best_score < STRONG_THRESHOLD:
        # print("STRONG FAISS MATCH") # Hide this print statement
        context = documents[indices[0]]

    # ---------------- WEAK MATCH ----------------
    elif best_score < WEAK_THRESHOLD:
        # print(" WEAK FAISS MATCH (top-3)") # Hide this print statement
        context = "\n\n".join(
            documents[i] for i in indices if i != -1
        )

    # ---------------- NO MATCH ----------------
    else:
        # print(" NO FAISS MATCH") # Hide this print statement
        return {
            "db_result": None,
            "search_distance": best_score,
            "next_step": "fallback"
        }

    # ---------------- RAG ----------------
    prompt = f"""
You are a Government Services Assistant for India.
Answer ONLY from the information below.

Information:
{context}

User Query:
{state["user_query"]}
"""

    final_answer = llm.invoke(prompt).content

    return {
        "db_result": context,
        "final_answer": final_answer,
        "search_distance": best_score,
        "next_step": "success"
    }

!pip install faiss-cpu

import faiss
import pickle
import numpy as np
from langchain_ollama import OllamaEmbeddings
import sqlite3

# Initialize embedding model
embedding_model = OllamaEmbeddings(model="nomic-embed-text")

# Connect to the SQLite database
conn = sqlite3.connect("dataset.db")
cursor = conn.cursor()

# Get actual column names from the database
cursor.execute("PRAGMA table_info(dataset)")
column_names = [col[1] for col in cursor.fetchall()]
print(f"Database columns: {column_names}")

# Map our desired information to the actual column names in the DB
mapping = {
    'service_name': 'service_name',
    'department': 'department',
    'steps': 'steps_to_apply',
    'docs': 'required_documents',
    'time': 'processing_time',
    'fee': 'fee',
    'eligibility': 'Eligibility',
    'url': 'protal_url',
    'info': 'additional_information'
}

select_columns = [col for col in mapping.values() if col in column_names]

cursor.execute(f"SELECT {', '.join(select_columns)} FROM dataset")
rows = cursor.fetchall()
conn.close()

documents = []
embeddings_list = []

print(f"Indexing {len(rows)} services...")

# Process each row to create a document string
for row in rows:
    doc_parts = []
    for i, col_name in enumerate(select_columns):
        val = row[i] if row[i] else "N/A"
        doc_parts.append(f"{col_name.replace('_', ' ').title()}: {val}")
    doc_string = "\n".join(doc_parts)
    documents.append(doc_string)
    embeddings_list.append(embedding_model.embed_query(doc_string))

# Convert to FAISS index
embeddings_array = np.array(embeddings_list).astype("float32")
faiss.normalize_L2(embeddings_array)
d = embeddings_array.shape[1]
index = faiss.IndexFlatL2(d)
index.add(embeddings_array)

print("FAISS index rebuilt with correct database columns.")

import os
if not os.path.exists("/content/faiss_index/"):
    os.makedirs("/content/faiss_index/")

# Save the FAISS index and documents
faiss.write_index(index, "/content/faiss_index/index.faiss")
with open("/content/faiss_index/docs.pkl", "wb") as f:
    pickle.dump(documents, f)

print(" FAISS index and documents saved to /content/faiss_index/")

def ollama_fallback(state: AgentState) -> AgentState:
    prompt = f"""
    You are a Government Services Assistant for India.

    User Query:
    {state["user_query"]}

    If the exact service is not found in the database:
    1. Give general guidance
    2. Suggest contacting official department
    3. Mention official government portals if possible
    4. Answer in the SAME language as user (Hindi or English)
    """

    response = llm.invoke(prompt).content

    return {
        **state,
        "final_answer": response
    }

import sqlite3

def init_memory_db():
    conn = sqlite3.connect("chat_memory.db")
    cur = conn.cursor()
    cur.execute("""
    CREATE TABLE IF NOT EXISTS memory (
        session_id TEXT,
        role TEXT,
        content TEXT
    )
    """)
    conn.commit()
    conn.close()

def save_message(session_id, role, content):
    conn = sqlite3.connect("chat_memory.db")
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO memory VALUES ",
        (session_id, role, content)
    )
    conn.commit()
    conn.close()

def load_memory(session_id, limit=6):
    conn = sqlite3.connect("chat_memory.db")
    cur = conn.cursor()
    cur.execute("""
        SELECT role, content FROM memory
        WHERE session_id=?
        ORDER BY rowid DESC LIMIT ?
    """, (session_id, limit))
    rows = cur.fetchall()
    conn.close()
    return list(reversed(rows))

init_memory_db()

!pip install langdetect deep-translator gTTS
import uuid
from langdetect import detect
from deep_translator import GoogleTranslator
from gtts import gTTS

# Helper for Speech-to-Text (Placeholder)
def speech_to_text(audio_path):
    # This is a placeholder; you could integrate OpenAI Whisper or SpeechRecognition here
    return "User voice input detected"

# Helper for Text-to-Speech
def text_to_speech(text, lang):
    tts = gTTS(text=text, lang=lang)
    temp_file = f"response_{uuid.uuid4()}.mp3"
    tts.save(temp_file)
    return temp_file

session_id = str(uuid.uuid4())

def chat_handler(text_input, audio_input):

    # -------- Input --------
    user_query = text_input.strip() if text_input else speech_to_text(audio_input)
    if not user_query:
        return "", " Please speak or type a query.", None, ""

    # -------- Language --------
    try:
        user_lang = detect(user_query)
    except:
        user_lang = "en"

    # -------- Translate for retrieval --------
    retrieval_query = (
        GoogleTranslator(source="auto", target="en").translate(user_query)
        if user_lang == "hi"
        else user_query
    )

    # -------- Load conversation memory --------
    history = load_memory(session_id)
    context_text = "\n".join([f"{r}: {c}" for r, c in history])

    final_query = f"{context_text}\nUser: {retrieval_query}"

    # -------- Call LangGraph (FULL QUERY SEARCH) --------
    result = app.invoke({"user_query": final_query})

    answer_en = result.get("final_answer", "No information found from dataset.")
    matched_service = result.get("db_result", "N/A")

    # -------- Translate back --------
    final_answer = (
        GoogleTranslator(source="auto", target="hi").translate(answer_en)
        if user_lang == "hi"
        else answer_en
    )

    # -------- Save memory --------
    save_message(session_id, "User", retrieval_query)
    save_message(session_id, "Bot", answer_en)

    # -------- Voice --------
    voice = text_to_speech(final_answer, "hi" if user_lang == "hi" else "en")

    return user_query, final_answer, voice, matched_service

from langgraph.graph import StateGraph, END

def info_extractor(state: AgentState):
    prompt = f"Extract the specific name of the government service from this query: '{state['user_query']}'. Return only the name or 'NONE'."
    response = llm.invoke(prompt).content.strip()
    return {"application_name": None if "NONE" in response.upper() else response}

def route_after_extraction(state: AgentState):
    if state.get("application_name"):
        return "search_database"
    return "ollama_fallback"

def route_after_search(state: AgentState):
    # If distance is low enough, we consider it a good match
    if state.get("db_result") and state.get("search_distance", 2.0) < 0.8:
        return END
    return "ollama_fallback"

# State Graph Construction
workflow = StateGraph(AgentState)

workflow.add_node("info_extractor", info_extractor)
workflow.add_node("search_database", search_database)
workflow.add_node("ollama_fallback", ollama_fallback)

workflow.set_entry_point("info_extractor")

workflow.add_conditional_edges("info_extractor", route_after_extraction)
workflow.add_conditional_edges("search_database", route_after_search)
workflow.add_edge("ollama_fallback", END)

app = workflow.compile()
print("LangGraph workflow compiled successfully.")

# Check search model dimensions
test_emb = embedding_model.embed_query("test")
print(f"Search Embedding Dimensions: {len(test_emb)}")

# Check index dimensions
print(f"FAISS Index Dimensions: {index.d}")

result = app.invoke({
    "user_query": "how to apply  driver licience"
})

print(result["final_answer"])

result = app.invoke({
    "user_query": "Driving license test fail ho jaye toh kya hota hai?"
})

print(result["final_answer"])

!pip install reportlab
import tempfile
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet

def generate_pdf(text):
    path = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf").name
    doc = SimpleDocTemplate(path)
    styles = getSampleStyleSheet()
    doc.build([Paragraph(text.replace("\n","<br/>"), styles["Normal"])])
    return path

!pip install gradio reportlab
import gradio as gr
import tempfile
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet

def generate_pdf(text):
    path = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf").name
    doc = SimpleDocTemplate(path)
    styles = getSampleStyleSheet()
    doc.build([Paragraph(text.replace("\n","<br/>"), styles["Normal"])])
    return path

with gr.Blocks(theme=gr.themes.Soft()) as demo:

    gr.Markdown("""
    <h1 style="text-align:center;"> Government Services Voice Assistant</h1>
    <p style="text-align:center;"> Hindi & English</p>
    <hr>
    """)

    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(label= "Type query", lines=3)
            audio_input = gr.Audio(sources=["microphone"], type="filepath", label=" Speak")
            ask_btn = gr.Button("Ask")

        with gr.Column():
            user_out = gr.Textbox(label="You asked")
            bot_out = gr.Textbox(label="Government Response", lines=8)
            service_out = gr.Textbox(label="Detected Service")
            audio_out = gr.Audio(label="Voice Reply")
            pdf_btn = gr.File(label=" Download PDF")

    ask_btn.click(
        chat_handler,
        inputs=[text_input, audio_input],
        outputs=[user_out, bot_out, audio_out, service_out]
    ).then(
        generate_pdf,
        inputs=bot_out,
        outputs=pdf_btn
    )

demo.launch(share=True)